# 数据标注平台测试方案

## 测试概述

本文档提供了Streamlit数据标注平台的完整测试方案，包括功能测试、性能测试、用户体验测试和集成测试。

## 1. 测试环境准备

### 1.1 环境要求

```bash
# Python 环境
Python >= 3.8

# 依赖包安装
pip install streamlit pandas sqlite3 uuid pathlib openpyxl
```

### 1.2 测试数据准备

测试数据已准备在 `test_data/` 目录下：

```
test_data/
├── sql_sample.jsonl              # SQL语义一致性测试数据
├── math_sample.jsonl             # 数学题标注测试数据  
├── deepresearch_sample.jsonl     # 研究报告评估测试数据
├── reports/                      # 示例报告文件
│   └── ai_medical_diagnosis.md
├── scores/                       # 模型评分文件
│   └── ai_medical_model_score.json
└── rubrics/                      # 评分标准文件
    └── research_evaluation.jsonl
```

### 1.3 启动应用

```bash
# 在项目根目录运行
streamlit run streamlit_app.py
```

应用将在 http://localhost:8501 启动

## 2. 功能测试用例

### 2.1 任务配置测试

#### 测试用例 TC001: 文件上传功能
**测试目标**: 验证JSONL文件上传和解析功能

**测试步骤**:
1. 导航到"任务配置"页面
2. 上传 `test_data/sql_sample.jsonl` 文件
3. 检查文件解析结果

**预期结果**:
- ✅ 成功显示"成功加载 10 条数据"
- ✅ 数据预览表格显示正确的字段和内容
- ✅ 检测到字段: question, sql, result

**测试数据**: `test_data/sql_sample.jsonl`

#### 测试用例 TC002: 字段配置功能
**测试目标**: 验证字段类型配置和预览功能

**测试步骤**:
1. 完成文件上传（TC001）
2. 选择字段: question, sql, result
3. 配置字段类型:
   - question: text
   - sql: code (语言: sql)
   - result: text
4. 查看配置预览

**预期结果**:
- ✅ 字段选择正确保存
- ✅ SQL代码带语法高亮显示
- ✅ 文本字段正确渲染
- ✅ 预览界面展示正确

#### 测试用例 TC003: 标注配置功能
**测试目标**: 验证不同标注形式的配置

**测试步骤**:
1. 完成字段配置（TC002）
2. 配置标注类型为"单选"
3. 设置选项: "True", "False"
4. 添加标注说明
5. 查看预览效果

**预期结果**:
- ✅ 标注类型正确设置
- ✅ 选项列表正确显示
- ✅ 预览界面显示单选按钮
- ✅ 标注说明正确显示

#### 测试用例 TC004: 任务创建功能
**测试目标**: 验证完整任务创建流程

**测试步骤**:
1. 完成标注配置（TC003）
2. 输入任务名称: "SQL语义一致性测试"
3. 输入任务描述
4. 点击"创建任务"

**预期结果**:
- ✅ 任务创建成功
- ✅ 显示任务ID
- ✅ 数据文件正确保存到data/目录
- ✅ 数据库中任务记录正确创建

### 2.2 数据标注测试

#### 测试用例 TC005: 标注界面功能
**测试目标**: 验证标注界面的基本功能

**测试步骤**:
1. 导航到"数据标注"页面
2. 选择已创建的任务
3. 查看第一条数据的展示
4. 进行标注操作

**预期结果**:
- ✅ 任务列表正确显示
- ✅ 数据内容正确渲染
- ✅ 标注表单正常工作
- ✅ 进度信息正确显示

#### 测试用例 TC006: 标注保存功能
**测试目标**: 验证标注结果的保存和加载

**测试步骤**:
1. 在第一条数据上选择"True"
2. 点击"保存标注"
3. 切换到其他数据再回来
4. 验证标注是否保存

**预期结果**:
- ✅ 保存成功提示
- ✅ 重新加载时标注结果正确显示
- ✅ 数据库中标注记录正确
- ✅ 进度统计更新

#### 测试用例 TC007: 导航功能
**测试目标**: 验证数据间的导航功能

**测试步骤**:
1. 使用"下一条"按钮浏览数据
2. 使用"上一条"按钮返回
3. 使用跳转功能到第5条数据
4. 验证导航边界条件

**预期结果**:
- ✅ 导航按钮正常工作
- ✅ 边界条件正确处理（第一条/最后一条）
- ✅ 跳转功能正确
- ✅ 当前位置指示正确

### 2.3 进度管理测试

#### 测试用例 TC008: 进度统计功能
**测试目标**: 验证任务进度的统计和显示

**测试步骤**:
1. 导航到"进度管理"页面
2. 查看任务进度统计
3. 标注几条数据后重新查看
4. 验证进度更新

**预期结果**:
- ✅ 总体统计正确
- ✅ 任务详细进度正确
- ✅ 进度条显示正确
- ✅ 实时更新正常

### 2.4 结果导出测试

#### 测试用例 TC009: 导出功能测试
**测试目标**: 验证多种格式的数据导出

**测试步骤**:
1. 完成部分数据标注
2. 导航到"结果导出"页面
3. 选择JSON格式导出
4. 下载并验证文件内容

**预期结果**:
- ✅ 导出选项正确显示
- ✅ 数据预览正确
- ✅ 文件下载成功
- ✅ 导出内容格式正确

## 3. 多种数据类型测试

### 3.1 数学题PDF标注测试

#### 测试用例 TC010: 数学题标注流程
**测试数据**: `test_data/math_sample.jsonl`

**测试步骤**:
1. 上传数学题测试数据
2. 配置字段类型:
   - problem: text
   - answer: text  
   - pdf_path: pdf
   - difficulty: text
3. 配置多级标注:
   - 是否需要丢弃 (单选: 是/否)
   - 如果不丢弃，一致性判断 (单选: 一致/不一致)
   - 如果不一致，修订内容 (文本输入)

**预期结果**:
- ✅ PDF文件路径正确处理（即使文件不存在也应显示路径）
- ✅ 多级标注逻辑正确
- ✅ 条件显示功能正常

### 3.2 研究报告评估测试

#### 测试用例 TC011: 复杂文档标注
**测试数据**: `test_data/deepresearch_sample.jsonl`

**测试步骤**:
1. 上传研究报告数据
2. 配置字段类型:
   - question: text
   - report_path: markdown
   - rubrics_path: text  
   - model_score_path: text
3. 配置评分标注 (1-10分)
4. 测试markdown渲染

**预期结果**:
- ✅ Markdown文件正确渲染
- ✅ 评分滑块正常工作
- ✅ 复杂数据结构正确处理

## 4. 性能测试

### 4.1 大文件测试

#### 测试用例 TC012: 大数据量处理
**测试目标**: 验证平台处理大量数据的能力

**测试步骤**:
1. 创建包含1000条数据的JSONL文件
2. 上传并配置任务
3. 测试标注界面响应速度
4. 测试导航性能

**生成测试数据**:
```python
# 生成大数据量测试文件
import json

data = []
for i in range(1000):
    data.append({
        "id": i,
        "question": f"这是第{i+1}个测试问题",
        "content": f"测试内容 {i+1}" * 10,  # 较长内容
        "category": f"类别{i % 5}"
    })

with open('test_data/large_dataset.jsonl', 'w') as f:
    for item in data:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')
```

**性能指标**:
- 文件上传时间 < 30秒
- 页面切换响应时间 < 2秒
- 标注保存响应时间 < 1秒

### 4.2 并发测试

#### 测试用例 TC013: 多用户模拟
**测试目标**: 模拟多用户同时使用系统

**测试方法**:
1. 在多个浏览器标签页中打开应用
2. 同时进行标注操作
3. 验证数据一致性

**预期结果**:
- ✅ 无数据冲突
- ✅ 性能无明显下降
- ✅ 用户操作独立

## 5. 错误处理测试

### 5.1 异常数据测试

#### 测试用例 TC014: 错误数据格式
**测试目标**: 验证错误处理机制

**测试数据**: 创建包含错误JSON的文件
```
{"valid": "data"}
{invalid json}
{"another": "valid"}
```

**预期结果**:
- ✅ 显示具体错误位置
- ✅ 不影响整个应用运行
- ✅ 提供有用的错误信息

### 5.2 文件路径测试

#### 测试用例 TC015: 文件不存在处理
**测试目标**: 验证文件路径错误的处理

**测试步骤**:
1. 上传包含不存在文件路径的数据
2. 配置相应字段为image/pdf类型
3. 查看错误提示

**预期结果**:
- ✅ 友好的错误提示
- ✅ 不阻断其他功能
- ✅ 提供替代方案

## 6. 用户体验测试

### 6.1 界面易用性测试

#### 测试用例 TC016: 新用户操作流程
**测试目标**: 验证新用户能否顺利完成整个流程

**测试方法**:
1. 邀请未接触过系统的用户
2. 提供基本说明
3. 观察操作过程和困难点
4. 记录用户反馈

**评估标准**:
- 任务配置完成时间 < 10分钟
- 标注操作学习时间 < 5分钟
- 用户满意度 > 4/5

### 6.2 响应式设计测试

#### 测试用例 TC017: 不同屏幕尺寸测试
**测试目标**: 验证界面在不同设备上的表现

**测试环境**:
- 桌面浏览器 (1920x1080)
- 平板设备 (768x1024)  
- 手机设备 (375x667)

**预期结果**:
- ✅ 界面元素正确缩放
- ✅ 文字清晰可读
- ✅ 操作按钮易于点击

## 7. 自动化测试脚本

### 7.1 基础功能自动化测试

```python
# test_automation.py
import streamlit as st
from streamlit.testing.v1 import AppTest
import json
import os
import tempfile

def test_file_upload():
    """测试文件上传功能"""
    app = AppTest.from_file("streamlit_app.py")
    app.run()
    
    # 创建测试文件
    test_data = [
        {"question": "测试问题", "answer": "测试答案"},
        {"question": "问题2", "answer": "答案2"}
    ]
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for item in test_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
        test_file_path = f.name
    
    # 模拟文件上传
    with open(test_file_path, 'rb') as f:
        app.file_uploader[0].upload(f)
    
    app.run()
    
    # 验证结果
    assert "成功加载 2 条数据" in app.get_text()
    
    # 清理
    os.unlink(test_file_path)

def test_annotation_save():
    """测试标注保存功能"""
    # 这里需要先创建任务，然后测试标注
    pass

if __name__ == "__main__":
    test_file_upload()
    print("✅ 文件上传测试通过")
```

### 7.2 数据库测试

```python
# test_database.py
import sqlite3
import tempfile
import os
from streamlit_app import DatabaseManager

def test_database_operations():
    """测试数据库基本操作"""
    # 使用临时数据库
    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:
        db_path = f.name
    
    try:
        # 初始化数据库
        db = DatabaseManager(db_path)
        
        # 测试任务创建
        task_data = {
            'name': '测试任务',
            'description': '这是一个测试任务',
            'config': {'test': 'config'},
            'data_path': 'test.jsonl'
        }
        
        task_id = db.create_task(task_data)
        assert task_id is not None
        
        # 测试任务获取
        task = db.get_task(task_id)
        assert task['name'] == '测试任务'
        
        # 测试标注保存
        db.save_annotation(task_id, 0, "测试结果")
        annotation = db.get_annotation(task_id, 0)
        assert annotation == "测试结果"
        
        # 测试进度获取
        progress = db.get_task_progress(task_id)
        assert progress['completed'] == 1
        
        print("✅ 数据库操作测试通过")
        
    finally:
        # 清理
        if os.path.exists(db_path):
            os.unlink(db_path)

if __name__ == "__main__":
    test_database_operations()
```

## 8. 测试执行计划

### 8.1 测试阶段

**第一阶段: 功能测试 (Day 1-2)**
- 执行 TC001-TC009 基础功能测试
- 修复发现的问题

**第二阶段: 多数据类型测试 (Day 2-3)**  
- 执行 TC010-TC011 复杂数据测试
- 验证所有数据类型支持

**第三阶段: 性能和错误处理测试 (Day 3-4)**
- 执行 TC012-TC015 性能和异常测试
- 优化性能瓶颈

**第四阶段: 用户体验测试 (Day 4-5)**
- 执行 TC016-TC017 UX测试
- 收集用户反馈并改进

### 8.2 测试报告模板

```markdown
# 测试执行报告

## 测试概要
- 测试时间: 
- 测试人员:
- 测试环境:

## 测试结果汇总
- 总测试用例数: 
- 通过用例数:
- 失败用例数:
- 通过率:

## 详细测试结果
| 用例ID | 用例名称 | 状态 | 备注 |
|--------|---------|------|------|
| TC001  | 文件上传功能 | ✅通过 | |
| TC002  | 字段配置功能 | ❌失败 | 预览功能异常 |

## 发现的问题
1. [高] 预览功能在某些情况下不工作
2. [中] 大文件上传时间过长
3. [低] 界面文字对齐问题

## 建议
1. 优化预览功能的错误处理
2. 增加上传进度指示
3. 调整CSS样式
```

## 9. 测试清单

在发布前确保以下所有项目都已测试通过：

### 功能完整性 ✅
- [ ] 文件上传和解析
- [ ] 字段类型配置  
- [ ] 标注形式配置
- [ ] 任务创建
- [ ] 数据标注
- [ ] 进度查看
- [ ] 结果导出

### 数据类型支持 ✅
- [ ] 文本数据渲染
- [ ] 代码高亮显示
- [ ] 图片文件显示
- [ ] PDF文件处理
- [ ] Markdown渲染

### 标注形式支持 ✅
- [ ] 单选功能
- [ ] 多选功能
- [ ] 评分功能
- [ ] 文本输入功能

### 性能要求 ✅
- [ ] 1000条数据处理正常
- [ ] 页面响应时间 < 3秒
- [ ] 文件上传 < 30秒

### 错误处理 ✅
- [ ] 文件格式错误提示
- [ ] 文件路径错误处理
- [ ] 数据库操作异常处理
- [ ] 用户输入验证

### 用户体验 ✅
- [ ] 界面直观易用
- [ ] 错误信息友好
- [ ] 操作反馈及时
- [ ] 帮助信息充分

通过完整的测试方案执行，可以确保Streamlit原型的质量和可用性，为后续的生产级开发提供可靠的基础。
